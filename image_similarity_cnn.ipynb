{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apparel Recommendations usin Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the feature vectors of all apparel images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "   Running this cell will take time, you can skip running this cell. you can download the feature vectors from given link\n",
    "   16k_data_cnn_features.npy: https://drive.google.com/open?id=0BwNkduBnePt2c1BkNzRDQ1dOVFk \n",
    "   bottleneck_features_cnn.npy : https://drive.google.com/open?id=0BwNkduBnePt2ODRxWHhUVzIyWDA\n",
    "</pre>"
   ]
  },
  
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras import applications\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    
   ],
   "source": [
    "# https://gist.github.com/fchollet/f35fbc80e066a49d65f1688a7e99f069\n",
    "# https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 224, 224\n",
    "\n",
    "top_model_weights_path = 'bottleneck_fc_model.h5'\n",
    "train_data_dir = 'images2/'\n",
    "nb_train_samples = 16042\n",
    "epochs = 50\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "def save_bottlebeck_features():\n",
    "    asins = []\n",
    "    datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    \n",
    "    # build the VGG16 network\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    generator = datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(img_width, img_height),\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False)\n",
    "\n",
    "    for i in generator.filenames:\n",
    "        asins.append(i[2:-5])\n",
    "\n",
    "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples // batch_size)\n",
    "    bottleneck_features_train = bottleneck_features_train.reshape((16042,25088))\n",
    "    \n",
    "    np.save(open('workshop/models/16k_data_cnn_features.npy', 'wb'), bottleneck_features_train)\n",
    "    np.save(open('workshop/models/16k_data_cnn_feature_asins.npy', 'wb'), np.array(asins))\n",
    "    \n",
    "\n",
    "save_bottlebeck_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load the extracted features"
   ]
  },
  
   ],
   "source": [
    "bottleneck_features_train = np.load('workshop/models/16k_data_cnn_features.npy')\n",
    "asins = np.load('workshop/models/16k_data_cnn_feature_asins.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the most similar apparels using euclidean distance measure"
   ]
  },
  
   
   "source": [
    "data = pd.read_pickle('workshop/pickels/16k_apperal_data_preprocessed')\n",
    "df_asins = list(data['asin'])\n",
    "asins = list(asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asins' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b61edf013656>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Amazon Url: www.amzon.com/dp/'\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0masins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mget_similar_products_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12566\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-b61edf013656>\u001b[0m in \u001b[0;36mget_similar_products_cnn\u001b[1;34m(doc_id, num_results)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_similar_products_cnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_results\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdoc_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_asins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mpairwise_dist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbottleneck_features_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbottleneck_features_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'asins' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image, SVG, Math, YouTubeVideo\n",
    "\n",
    "def get_similar_products_cnn(doc_id, num_results):\n",
    "    doc_id = asins.index(df_asins[doc_id])\n",
    "    pairwise_dist = pairwise_distances(bottleneck_features_train, bottleneck_features_train[doc_id].reshape(1,-1))\n",
    "\n",
    "    indices = np.argsort(pairwise_dist.flatten())[0:num_results]\n",
    "    pdists  = np.sort(pairwise_dist.flatten())[0:num_results]\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        rows = data[['medium_image_url','title']].loc[data['asin']==asins[indices[i]]]\n",
    "        for indx, row in rows.iterrows():\n",
    "            display(Image(url=row['medium_image_url'], embed=True))\n",
    "            print('Product Title: ', row['title'])\n",
    "            print('Euclidean Distance from input image:', pdists[i])\n",
    "            print('Amazon Url: www.amzon.com/dp/'+ asins[indices[i]])\n",
    "\n",
    "get_similar_products_cnn(12566, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with GPU => per image 3.5/20 (0.175 sec), for whole data set its taking around 40min\n",
    "# with CPU => per image 12.3/20 (0.615 sec), for whole data set its taking around 160min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
